Attention is all you need

### 定义
Transformer是一种基于注意力机制的深度学习模型，最初由Vaswani等人在2017年的论文《Attention is All You Need》中提出。它在自然语言处理（NLP）领域取得了革命性的进展，特别是在序列到序列的任务中，如机器翻译。

### Transformer的主要特点
- **不依赖循环神经网络**（RNN）或卷积神经网络（CNN）
- **完全基于注意力机制**来处理序列数据
- **高效的并行处理能力**和**较少的训练时间**

### Transformer模型结构
Transformer模型主要包含两大部分：编码器（Encoder）和解码器（Decoder）。
- **编码器**: 由多个相同的层堆叠而成，每层包含两个子层（自注意力机制和前馈神经网络）。
- **解码器**: 也由多个相同的层堆叠而成，但每层有三个子层（自注意力机制、编码器-解码器注意力机制和前馈神经网络）。

### 注意力机制
注意力机制允许模型在处理每个单词时动态地关注输入序列的不同部分。

#### 自注意力（Self-Attention）
- **作用**: 让模型在编码器中学习输入序列内各个位置之间的关系。
- [[Self-Attention]]
- **计算过程**:
  1. 将输入序列的每个元素转换为三个向量：查询向量（Query）、键向量（Key）和值向量（Value）。
  2. 计算注意力分数：`Score = Query * Key^T`
  3. 应用softmax函数，得到权重分布：`Attention_weights = softmax(Score)`
  4. 用权重分布加权值向量：`Output = Attention_weights * Value`

#### 编码器-解码器注意力（Encoder-Decoder Attention）
- **作用**: 帮助解码器关注输入序列的相关部分。
- **计算过程**: 类似于自注意力，但查询向量来自于前一解码器层的输出

### Transformer的关键公式
- 注意力计算：`Attention(Q, K, V) = softmax(QK^T / √d_k) V`
  - 其中`Q`、`K`、`V`分别是查询（Query）、键（Key）和值（Value）矩阵，`d_k`是键向量的维度。

### 前馈神经网络
这一层把神经元数量先扩大再缩小，像一个“知识库” database

### Positional Encoding
#### 概述
- Transformer 缺乏自然处理顺序信息的能力，因此需要位置编码来辅助。
- 位置编码通过向每个单词的嵌入向量中添加位置信息来实现。

#### 编码方式
- 使用不同频率的正弦和余弦函数来生成位置编码。
- 公式：
  $$ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i / d_{model}}}\right) $$
  $$ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i / d_{model}}}\right) $$

#### 重要性
- 使模型能够识别并利用序列中的位置信息。
- 在不同的维度上捕捉不同粒度的位置依赖关系。
> 对于较低的维度，模型可能更关注单词之间的近邻关系，而对于较高的维度，则可能关注更长范围内的序列模式。这种多频率的方法使得 Transformer 能够同时考虑序列中短距离和长距离的依赖关系，从而提高了其处理序列数据的能力。



### Transformer在NLP中的应用
- **机器翻译**
- **文本摘要**
- **问答系统**
- **语言模型预训练**，如BERT和GPT系列

### 总结
Transformer模型由于其独特的注意力机制和并行处理能力，在NLP领域取得了巨大的成功。其架构被广泛应用于各种语言处理任务，并激发了许多后续的研究和模型创新。
