> AIGC
# 均匀分布（Uniform Distribution）

#### 定义
均匀分布是一种概率分布，其中每个数值在一定范围内出现的概率是等同的。在初始化神经网络权重时，均匀分布初始化意味着权重是从一个定义的范围（如[a, b]）内均匀随机选取的。

#### 公式
对于一个定义在[a, b]区间的均匀分布，其概率密度函数（PDF）为：
$$ f(x) = \frac{1}{b - a} \quad \text{对于} \quad a \leq x \leq b $$

#### 特点
- 每个数值在指定区间内出现的概率相等。
- 常用于神经网络的权重初始化，尤其是当不清楚权重应该使用什么分布时。

# 正态分布（Normal Distribution）

#### 定义
正态分布，也称为高斯分布，是一种普遍存在的概率分布。它由两个参数定义：均值（μ）和标准差（σ）。

#### 公式
正态分布的概率密度函数为：
$$ f(x | \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2} \left(\frac{x - \mu}{\sigma}\right)^2} $$

#### 特点
- 曲线呈钟形，关于均值对称。
- 在神经网络中，正态分布初始化可以帮助权重分布在一个合理的范围，防止梯度过大或过小。

# 常数（Constant）

#### 定义
常数初始化是将神经网络的所有权重初始化为同一个常数值，如0或1。

#### 特点
- 简单且直观。
- 通常不推荐使用，因为这可能导致神经元学习相同的特征。

# 随机（Random）

#### 定义
随机初始化是指从某个概率分布（如均匀分布或正态分布）中随机选择权重。这是神经网络中常用的初始化方法。

#### 特点
- 通过随机化，每个神经元可以学习到不同的特征。
- 可以防止模型在训练开始时就陷入对称性，提高训练效果。


# 凯明初始化（Kaiming He初始化）

#### 定义
凯明初始化，也称为He初始化，是一种网络权重初始化方法，由何恺明（Kaiming He）等人提出。这种方法特别适用于ReLU（Rectified Linear Unit）激活函数的深层网络，目的是在训练开始时防止梯度消失或爆炸。

#### 公式
在凯明初始化中，权重w按照正态分布随机初始化，其均值为0，标准差为：
$$ \text{std} = \sqrt{\frac{2}{n}} $$
其中，\( n \) 代表层中的输入单元数。

#### 特点
- 适用于ReLU激活函数，因为ReLU在正输入时的导数为1，这有助于缓解梯度消失的问题。
- 适合深层网络，可以在训练初期防止梯度消失或爆炸。

# Xavier/Glorot初始化

#### 定义
Xavier初始化，也称为Glorot初始化，是由Xavier Glorot等人提出的一种权重初始化方法。它适用于tanh和sigmoid激活函数，旨在训练开始时保持输入和输出的方差一致，从而避免梯度消失或爆炸。

#### 公式
在Xavier初始化中，权重w按照均匀分布或正态分布随机初始化，其中均匀分布的范围是[-r, r]，r计算公式为：
$$ r = \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}} $$
其中，$n_{\text{in}}$ 和 $n_{\text{out}}$ 分别是层的输入和输出单元数。

如果使用正态分布，那么标准差为：
$$ \text{std} = \sqrt{\frac{2}{n_{\text{in}} + n_{\text{out}}}} $$

#### 特点
- 适用于tanh和sigmoid激活函数。
- 能在网络的初期阶段维持激活值和梯度的方差，防止它们过大或过小。
