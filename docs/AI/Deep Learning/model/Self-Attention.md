# 过程
在 Transformer 模型中，自注意力机制的计算公式涉及到 Query、Key 和 Value。以下是自注意力计算的标准步骤：

1. **计算 Query、Key、Value**：
   $$Q = XW^Q, \quad K = XW^K, \quad V = XW^V$$
   这里，$X$ 是输入的嵌入表示，而 $W^Q$、$W^K$ 和 $W^V$ 是模型参数（权重矩阵），分别用于产生 Query、Key 和 Value。
   $d_{model}$ 权重矩阵的维度，也代表着模型的维度
   Transformer所有层中的数据都保持着相同的维度，从而可以处理不同长度的输入序列

2. **计算注意力分数**：
   $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
   在这里，$QK^T$ 表示 Query 和 Key 的点积，用于衡量它们之间的相似度。然后通过 $d_k$ （Key 的维度）的平方根来进行缩放，防止点积过大导致梯度消失问题。softmax 函数用于将这些分数转换为概率分布。

3. **应用 softmax 函数**：
   $$ \text{softmax}(x_i) = \frac{\exp(x_i)}{\sum_j \exp(x_j)} $$
   对于每个 Query，softmax 函数确保所有对应的 Key 的分数相加为 1，这样就形成了一个概率分布，表明每个 Value 对当前 Query 的贡献程度。

4. **加权和得到输出**：
   上面的注意力分数与 Value 相乘后求和，得到的输出就是加权后的 Value 的集合，这个输出会用于下一层或者其他处理步骤。

整个自注意力机制的核心在于允许模型在处理每个单词时，都能考虑到输入序列中的所有单词，并以此动态调整对不同单词的关注程度。这种机制使 Transformer 模型特别适合处理序列任务，如机器翻译、文本摘要等。

# 举例

假设我们有一个简单的输入序列 $X$，它包含两个单词 $x_1$ 和 $x_2$，**每个单词由一个4维的向量表示。**我们计算 Query (Q), Key (K), 和 Value (V) 如下：

$$ Q = XW^Q, \quad K = XW^K, \quad V = XW^V $$

这里 $W^Q$, $W^K$, 和 $W^V$ 是模型学习到的权重矩阵。现在，我们将**简化为每个权重矩阵是单位矩阵**，这意味着 $Q$, $K$, 和 $V$ 实际上等于输入 $X$。

接着，我们计算 Query 与所有 Key 的点积，得到注意力分数：

$$ \text{Scores} = QK^T $$

然后，我们通过 softmax 函数将这些分数转换为概率：

$$ \text{Attention Weights} = \text{softmax}\left(\frac{\text{Scores}}{\sqrt{d_k}}\right) $$

在我们的例子中，假设 $d_k = 4$。这个点积被缩放了 $\sqrt{d_k}$，防止分数过大。然后，我们将注意力权重应用到 Value 上，得到最终的输出：

$$ \text{Output} = \text{Attention Weights} \cdot V $$

在具体的例子中，我们假设 $x_1$ 和 $x_2$ 是两个4维向量：

$x_1 = [1, 0, 1, 0]$
$x_2 = [0, 1, 0, 1]$

点积 $QK^T$ 会是：

$$ QK^T = \begin{bmatrix} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \end{bmatrix} \cdot \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} $$

应用 softmax 函数（假设没有缩放）得到：

$$ \text{softmax}([2, 0]) \approx [0.88, 0.12] $$
$$ \text{softmax}([0, 2]) \approx [0.12, 0.88] $$

最后，加权后的 Value （因为 $V = XW^V$,$W^V=I$）会是：

对于 $x_1$ 的输出：

$$ [0.88, 0.12] \cdot \begin{bmatrix} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \end{bmatrix} = [0.88, 0, 0.88, 0] $$

对于 $x_2$ 的输出：

$$ [0.12, 0.88] \cdot \begin{bmatrix} 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \end{bmatrix} = [0, 0.88, 0, 0.88] $$

这样，每个单词都得到了一个新的表示，它包含了整个序列中其他单词的信息，这一点是通过自注意力机制实现的。

**输入输出 单词向量的维度相同**

# 维度关系
# LLM 大的是$d_{model}$

> 在大型语言模型，尤其是基于 Transformer 架构的模型（如 GPT-3 或 BERT）中，模型的“大”主要体现在隐藏层的维度（通常表示为 $d_{model}$​）上。这个维度决定了自注意力机制中权重矩阵的大小，以及整个模型的参数总量。
> 在 Transformer 模型中， $d_{model}$​是一个关键的参数，因为它直接影响：
> 1. **模型的能力**：更大的  $d_{model}$​意味着模型可以学习和表示更复杂的特征和模式。这对于理解和生成复杂的自然语言非常重要。
> 2. **计算资源的需求**：随着  $d_{model}$​的增加，模型的参数数量急剧增加，这导致需要更多的计算资源来训练和运行模型。
> 3. **模型的参数数量**：在 Transformer 架构中，绝大多数参数都存在于自注意力层和前馈网络层的权重矩阵中。增加  $d_{model}$​会线性地增加这些矩阵的大小，从而增加整个模型的参数数量。
> 
> 因此，当我们说一个语言模型是“大型”模型时，我们通常是指它拥有较大的隐藏层维度和相应的高参数量，这使得模型能够捕获更丰富的语言特征和学习更复杂的模式。然而，这也意味着更高的计算成本和更复杂的模型管理。

同时自注意力层数也很大！每层都有真么多的维度。
GPT3 96层注意力。

# Multi-head Attention
每一层的QKV，其实是有多个QKV，每个的维度是$d_{head}$, 有$n_{head}$个头
$d_{head} * n_{head}=d_{model}$
最后用一个权重矩阵把多个头拼到一起
使用多头注意力机制的话，每层参数量为$4d_{model}$


