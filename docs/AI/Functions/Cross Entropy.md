## 交叉熵损失函数概述
交叉熵（Cross Entropy）是一种在机器学习中常用的损失函数，特别是在分类问题中。它衡量的是两个概率分布之间的差异，通常用于评估模型预测的概率分布与真实标签的概率分布之间的不一致性。

> Pytorch 中 CrossEntrypyLoss implicitly applies a softmax activation followed by a log transformation but NLLLoss does not.

## 基本原理
交叉熵的公式为：

$$
H(y, \hat{y}) = -\sum_{i} y_i \log(\hat{y_i})
$$

其中，$y$是真实标签的概率分布，$\hat{y}$是模型预测的概率分布，$y_i$和$\hat{y_i}$分别是这两个分布中的元素。公式中的求和是在所有可能的类别上进行的。

## 应用场景
交叉熵损失函数主要用于分类问题，尤其是多类分类问题。它通过比较模型预测的概率分布和真实的概率分布来计算损失，使得模型可以更好地进行概率预测。

## 优点
1. **梯度优化**：在梯度下降算法中表现良好，有利于加速学习过程。
2. **区分度高**：对于正确分类和错误分类的区分度高，尤其是在输出概率接近0或1时。

## 缺点
1. **对数函数敏感性**：由于含有对数函数，当预测概率接近0时可能导致数值不稳定。
2. **标签需为概率分布**：要求标签是概率分布形式，对于某些特定类型的问题可能需要额外的处理。

## 与其他损失函数的比较
与平方误差损失（Mean Squared Error）等其他损失函数相比，交叉熵损失在分类问题中更为常用。它在处理概率输出时更为高效，尤其是在输出层使用Softmax函数的情况下。

## 实际应用
在神经网络中，尤其是深度学习模型中，交叉熵损失函数被广泛用于训练过程中，以优化模型的参数，提高分类的准确性。
